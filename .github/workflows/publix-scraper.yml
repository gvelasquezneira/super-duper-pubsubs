name: Publix Data Collection

on:
  workflow_dispatch:
  schedule:
    - cron: '1 6 * * 2'

jobs:
  setup:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.create-matrix.outputs.matrix }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Split locations into parts
        id: create-matrix
        run: |
          mkdir -p temp_locations
          
          # Count total number of locations (excluding header)
          TOTAL=$(tail -n +2 publix_locations.csv | wc -l)
          echo "Total locations: $TOTAL"
          
          # Calculate lines per file (ceiling division)
          PER_FILE=$(( (TOTAL + 19) / 20 ))
          echo "Locations per file: $PER_FILE"
          
          # Split the file with fixed naming
          head -n 1 publix_locations.csv > header.csv
          tail -n +2 publix_locations.csv > data.csv
          
          # Split data into parts (up to 20)
          split -l $PER_FILE data.csv temp_locations/part_
          
          # Add header to each part
          for file in temp_locations/part_*; do
            cat header.csv "$file" > "$file.csv"
            rm "$file"  # Remove the headerless file
            mv "$file.csv" "$file"
            echo "Created $file with $(wc -l < "$file") lines"
          done
          
          # Create a simple fixed matrix output
          # This avoids issues with dynamic JSON construction
          echo "matrix={\"part\":[\"aa\",\"ab\",\"ac\",\"ad\",\"ae\",\"af\",\"ag\",\"ah\",\"ai\",\"aj\",\"ak\",\"al\",\"am\",\"an\",\"ao\",\"ap\",\"aq\",\"ar\",\"as\",\"at\"]}" >> $GITHUB_OUTPUT
          
          # List created files
          ls -la temp_locations/
      - name: Upload location parts
        uses: actions/upload-artifact@v4
        with:
          name: location-parts
          path: temp_locations/
          retention-days: 1

  scrape:
    needs: setup
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix: ${{fromJson(needs.setup.outputs.matrix)}}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Download location parts
        uses: actions/download-artifact@v4
        with:
          name: location-parts
          path: temp_locations

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install playwright
          python -m playwright install chromium
          python -m playwright install-deps
      - name: Verify and process file
        run: |
          echo "Processing part ${{ matrix.part }}"
          
          # Find the file to process
          PART_FILE=$(ls temp_locations/part_${{ matrix.part }}* 2>/dev/null || echo "")
          
          if [ -z "$PART_FILE" ]; then
            echo "No file found for part ${{ matrix.part }}"
            echo "Available files:"
            ls -la temp_locations/
            exit 0  # Skip this job if no file exists
          fi
          
          echo "Found file: $PART_FILE"
          echo "Number of locations: $(tail -n +2 "$PART_FILE" | wc -l)"
          
          # Copy to input locations
          cp "$PART_FILE" input_locations.csv
          
          # Run scraper with this part's locations
          python publix.py --input_file input_locations.csv --output_file "results_${{ matrix.part }}.csv" --workers 1 --headless true
          
          # Check results
          if [ -f "results_${{ matrix.part }}.csv" ]; then
            echo "Results file created with $(wc -l < results_${{ matrix.part }}.csv) lines"
          else
            echo "Warning: No results file was created!"
            # Create empty file to avoid errors in later steps
            echo "ID,Store ID,Location,Category,Product Name,Price,Size,Date" > "results_${{ matrix.part }}.csv"
          fi
      - name: Upload part results
        uses: actions/upload-artifact@v4
        with:
          name: publix-data-${{ matrix.part }}
          path: results_${{ matrix.part }}.csv
          retention-days: 30

  combine:
    needs: scrape
    runs-on: ubuntu-latest
    
    steps:
      - name: Download all results
        uses: actions/download-artifact@v4
        with:
          path: ./results
          pattern: publix-data-*
          merge-multiple: true
      
      - name: Combine results
        run: |
          mkdir -p combined
          DATE=$(date +%Y%m%d)
          
          # List all result files
          echo "Found result files:"
          find ./results -name "*.csv" | sort
          
          # Create header in output file
          echo "ID,Store ID,Location,Category,Product Name,Price,Size,Date" > "combined/publix_targeted_data_${DATE}_combined.csv"
          
          # Append data from all files (skipping headers)
          for file in $(find ./results -name "*.csv" | sort); do
            if [ -f "$file" ]; then
              echo "Processing $file"
              tail -n +2 "$file" >> "combined/publix_targeted_data_${DATE}_combined.csv"
            fi
          done
          
          # Count results
          TOTAL_LINES=$(wc -l < "combined/publix_targeted_data_${DATE}_combined.csv")
          DATA_LINES=$((TOTAL_LINES - 1))  # Subtract header line
          echo "Combined file created with $TOTAL_LINES lines ($DATA_LINES data rows)"
          
          # Count unique locations
          UNIQUE_LOCATIONS=$(tail -n +2 "combined/publix_targeted_data_${DATE}_combined.csv" | cut -d',' -f3 | sort | uniq | wc -l)
          echo "Unique locations in combined results: $UNIQUE_LOCATIONS"
      - name: Upload combined results
        uses: actions/upload-artifact@v4
        with:
          name: publix-combined-data
          path: combined/publix_targeted_data_*_combined.csv
          retention-days: 90
