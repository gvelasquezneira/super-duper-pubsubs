name: Publix Data Collection

on:
  schedule:
    - cron: '1 6 * * 2'  # Runs at 6:01 AM every Tuesday
  workflow_dispatch:  # Allows manual triggering

jobs:
  setup:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.split-locations.outputs.matrix }}
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          
      - name: Split locations file into 8 parts
        id: split-locations
        run: |
          # Create a temporary directory
          mkdir -p temp_locations
          
          # Count total number of locations (excluding header)
          TOTAL=$(tail -n +2 publix_locations.csv | wc -l)
          
          # Calculate lines per file (ceiling division)
          PER_FILE=$(( (TOTAL + 7) / 8 ))
          
          # Split the file (keeping header in each file)
          head -n 1 publix_locations.csv > header.csv
          tail -n +2 publix_locations.csv > data.csv
          split -l $PER_FILE data.csv temp_locations/part_
          
          # Add header to each split file
          for file in temp_locations/part_*; do
            cat header.csv "$file" > "${file}_with_header.csv"
          done
          
          # Create matrix output
          echo "matrix={\"part\":[1,2,3,4,5,6,7,8]}" >> $GITHUB_OUTPUT

  scrape:
    needs: setup
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix: ${{fromJson(needs.setup.outputs.matrix)}}
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install playwright
          pip install csv
          python -m playwright install chromium
          python -m playwright install-deps
          
      - name: Create part-specific locations file
        run: |
          # Copy the appropriate part file to use as publix_locations.csv
          cp temp_locations/part_*${matrix.part}*_with_header.csv publix_locations.csv
          
          # Check how many locations we're processing
          echo "Processing these locations for part ${matrix.part}:"
          cat publix_locations.csv
          
      - name: Run Publix scraper
        run: |
          python publix.py --workers 1
          
      - name: Upload results
        uses: actions/upload-artifact@v3
        with:
          name: publix-data-part-${{ matrix.part }}
          path: publix_targeted_data_*.csv
          retention-days: 30

  combine:
    needs: scrape
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          
      - name: Download all artifacts
        uses: actions/download-artifact@v3
        with:
          path: artifacts
          
      - name: Combine results
        run: |
          # Create output directory
          mkdir -p combined
          
          # Get today's date in YYYYMMDD format
          DATE=$(date +%Y%m%d)
          
          # Create a combined file with headers
          echo 'Location,Category,Product Name,Price,Ounces,Date' > combined/publix_targeted_data_${DATE}_combined.csv
          
          # Append all data files (skipping headers)
          for part in {1..8}; do
            FILE=$(find artifacts/publix-data-part-$part -name "publix_targeted_data_*.csv" -type f | head -1)
            if [ -f "$FILE" ]; then
              echo "Adding data from $FILE"
              tail -n +2 "$FILE" >> combined/publix_targeted_data_${DATE}_combined.csv
            fi
          done
          
          # Check the combined file
          echo "Combined file created with $(wc -l < combined/publix_targeted_data_${DATE}_combined.csv) lines"
          
      - name: Upload combined results
        uses: actions/upload-artifact@v3
        with:
          name: publix-combined-data
          path: combined/publix_targeted_data_*_combined.csv
          retention-days: 90
