name: Publix Data Collection (Staggered)

on:
  workflow_dispatch:
    inputs:
      team:
        description: 'Team to run (1, 2, or 3)'
        required: true
        default: '1'
        type: choice
        options:
          - '1'
          - '2'
          - '3'
  schedule:
    - cron: '0 9 * * 0'
    - cron: '30 3 * * 1' 
    - cron: '0 13 * * 1' 

jobs:
  determine-team:
    runs-on: ubuntu-latest
    outputs:
      team: ${{ steps.set-team.outputs.team }}
    steps:
      - id: set-team
        run: |
          # If manually triggered, use the input team
          if [[ "${{ github.event_name }}" == "workflow_dispatch" ]]; then
            echo "team=${{ github.event.inputs.team }}" >> $GITHUB_OUTPUT
          else
            # If scheduled, determine team based on the hour
            HOUR=$(date +%H)
            if [[ $HOUR -lt 6 ]]; then
              echo "team=1" >> $GITHUB_OUTPUT
            elif [[ $HOUR -lt 12 ]]; then
              echo "team=2" >> $GITHUB_OUTPUT
            else
              echo "team=3" >> $GITHUB_OUTPUT
            fi
          fi
      
      - name: Debug team selection
        run: |
          echo "Selected team: ${{ steps.set-team.outputs.team }}"

  setup:
    needs: determine-team
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.create-matrix.outputs.matrix }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Determine location range for this team
        id: create-matrix
        run: |
          TEAM="${{ needs.determine-team.outputs.team }}"
          echo "Processing team $TEAM"
          
          # Create Python script file instead of inline
          cat > split_locations.py << 'EOF'
          import csv
          import sys
          import os

          team = int(sys.argv[1])
          locations = []

          # Read all locations
          with open('publix_locations.csv', 'r') as f:
            reader = csv.reader(f)
            header = next(reader)  # Skip header
            locations = list(reader)

          total_locs = len(locations)
          # Each team handles 1/3 of the locations
          team_size = (total_locs + 2) // 3  # ceiling division

          # Calculate start and end for this team
          start_idx = (team - 1) * team_size
          end_idx = min(start_idx + team_size, total_locs)
          team_locs = locations[start_idx:end_idx]

          print(f'Team {team}: Processing locations {start_idx+1}-{end_idx} out of {total_locs}')

          # Create directory for split files
          os.makedirs('temp_locations', exist_ok=True)

          # Split into 5 parts within this team's allocation
          workers = 5
          locs_per_worker = (len(team_locs) + workers - 1) // workers  # ceiling division

          for i in range(workers):
              worker_start = i * locs_per_worker
              worker_end = min(worker_start + locs_per_worker, len(team_locs))
              
              worker_locs = team_locs[worker_start:worker_end]
              
              # Create file with header
              with open(f'temp_locations/part_{i+1}.csv', 'w', newline='') as f:
                  writer = csv.writer(f)
                  writer.writerow(header)
                  writer.writerows(worker_locs)
              
              print(f'Created part_{i+1}.csv with {len(worker_locs)} locations')
          EOF
          
          # Run the Python script with the team number
          python3 split_locations.py $TEAM
          
          # Create matrix output for 5 workers
          echo "matrix={\"part\":[1,2,3,4,5]}" >> $GITHUB_OUTPUT
          
      - name: Save team to file
        run: |
          echo "${{ needs.determine-team.outputs.team }}" > team_number.txt
          
      - name: Upload team info
        uses: actions/upload-artifact@v4
        with:
          name: team-info
          path: team_number.txt
          retention-days: 1
          
      - name: Upload location parts
        uses: actions/upload-artifact@v4
        with:
          name: location-parts
          path: temp_locations/
          retention-days: 1

  scrape:
    needs: setup
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix: ${{fromJson(needs.setup.outputs.matrix)}}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Download team info
        uses: actions/download-artifact@v4
        with:
          name: team-info
          
      - name: Get team number
        id: get-team
        run: |
          TEAM=$(cat team_number.txt)
          echo "team=$TEAM" >> $GITHUB_OUTPUT
          echo "Using team: $TEAM"

      - name: Download location parts
        uses: actions/download-artifact@v4
        with:
          name: location-parts
          path: temp_locations

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install playwright
          python -m playwright install chromium
          python -m playwright install-deps

      - name: Verify and process file
        run: |
          echo "Processing team ${{ steps.get-team.outputs.team }}, worker ${{ matrix.part }}"
          
          # Find the file to process
          PART_FILE=$(ls temp_locations/part_${{ matrix.part }}.csv 2>/dev/null || echo "")
          
          if [ -z "$PART_FILE" ]; then
            echo "No file found for part ${{ matrix.part }}"
            echo "Available files:"
            ls -la temp_locations/
            exit 1  # Fail if file not found
          fi
          
          echo "Found file: $PART_FILE"
          echo "Number of locations: $(tail -n +2 "$PART_FILE" | wc -l)"
          
          # Copy to input locations
          cp "$PART_FILE" input_locations.csv
          
          # Run scraper with this part's locations (with added delays)
          python publix.py --input_file input_locations.csv --output_file "results_team${{ steps.get-team.outputs.team }}_worker${{ matrix.part }}.csv" --workers 1 --headless true
          
          # Check results
          RESULTS_FILE="results_team${{ steps.get-team.outputs.team }}_worker${{ matrix.part }}.csv"
          if [ -f "$RESULTS_FILE" ]; then
            echo "Results file created with $(wc -l < $RESULTS_FILE) lines"
          else
            echo "Warning: No results file was created!"
            # Create empty file to avoid errors in later steps
            echo "ID,Store ID,Location,Category,Product Name,Price,Size,Date" > "$RESULTS_FILE"
          fi

      - name: Upload part results
        uses: actions/upload-artifact@v4
        with:
          name: publix-data-team${{ steps.get-team.outputs.team }}-worker${{ matrix.part }}
          path: results_team${{ steps.get-team.outputs.team }}_worker${{ matrix.part }}.csv
          retention-days: 30

  combine:
    needs: scrape
    runs-on: ubuntu-latest
    
    steps:
      - name: Download team info
        uses: actions/download-artifact@v4
        with:
          name: team-info
          
      - name: Get team number
        id: get-team
        run: |
          TEAM=$(cat team_number.txt)
          echo "team=$TEAM" >> $GITHUB_OUTPUT
          echo "Using team: $TEAM"
          
      - name: Download all results
        uses: actions/download-artifact@v4
        with:
          path: ./results
          pattern: publix-data-team${{ steps.get-team.outputs.team }}-worker*
          merge-multiple: true
      
      - name: Combine results
        run: |
          mkdir -p combined
          DATE=$(date +%Y%m%d)
          TEAM="${{ steps.get-team.outputs.team }}"
          
          # List all result files
          echo "Found result files:"
          find ./results -name "*.csv" | sort
          
          # Create header in output file
          echo "ID,Store ID,Location,Category,Product Name,Price,Size,Date" > "combined/publix_targeted_data_${DATE}_team${TEAM}_combined.csv"
          
          # Append data from all files (skipping headers)
          for file in $(find ./results -name "*.csv" | sort); do
            if [ -f "$file" ]; then
              echo "Processing $file"
              tail -n +2 "$file" >> "combined/publix_targeted_data_${DATE}_team${TEAM}_combined.csv"
            fi
          done
          
          # Count results
          TOTAL_LINES=$(wc -l < "combined/publix_targeted_data_${DATE}_team${TEAM}_combined.csv")
          DATA_LINES=$((TOTAL_LINES - 1))  # Subtract header line
          echo "Combined file created with $TOTAL_LINES lines ($DATA_LINES data rows)"
          
          # Count unique locations
          UNIQUE_LOCATIONS=$(tail -n +2 "combined/publix_targeted_data_${DATE}_team${TEAM}_combined.csv" | cut -d',' -f3 | sort | uniq | wc -l)
          echo "Unique locations in combined results: $UNIQUE_LOCATIONS"

      - name: Upload combined results
        uses: actions/upload-artifact@v4
        with:
          name: publix-combined-data-team${{ steps.get-team.outputs.team }}
          path: combined/publix_targeted_data_*_combined.csv
          retention-days: 90
