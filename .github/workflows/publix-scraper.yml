name: Publix Data Collection

on:
  workflow_dispatch:
  schedule:
    - cron: '1 6 * * 2'

jobs:
  setup:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.create-matrix.outputs.matrix }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Split locations into parts
        id: create-matrix
        run: |
          mkdir -p temp_locations
          
          # Count total number of locations (excluding header)
          TOTAL=$(tail -n +2 publix_locations.csv | wc -l)
          echo "Total locations: $TOTAL"
          
          # Calculate lines per file (ceiling division)
          PER_FILE=$(( (TOTAL + 19) / 20 ))
          echo "Locations per file: $PER_FILE"
          
          # Split the file with fixed naming
          head -n 1 publix_locations.csv > header.csv
          tail -n +2 publix_locations.csv > data.csv
          
          # Split data into parts
          split -l $PER_FILE data.csv part_
          
          # Add header to each part and move to temp_locations
          PARTS=0
          for file in part_*; do
            PARTS=$((PARTS+1))
            PART_NAME=$(printf "part_%02d" $PARTS)
            cat header.csv "$file" > "temp_locations/locations_${PART_NAME}.csv"
            echo "Created temp_locations/locations_${PART_NAME}.csv with $(wc -l < "temp_locations/locations_${PART_NAME}.csv") lines"
          done
          
          # Create matrix JSON for dynamic job creation
          echo "matrix={\"part\":[" >> $GITHUB_OUTPUT
          for i in $(seq 1 $PARTS); do
            PART_NAME=$(printf "part_%02d" $i)
            if [ $i -eq $PARTS ]; then
              echo "\"$PART_NAME\"" >> $GITHUB_OUTPUT
            else
              echo "\"$PART_NAME\"," >> $GITHUB_OUTPUT
            fi
          done
          echo "]}" >> $GITHUB_OUTPUT
          
          # Verify the output
          echo "Generated matrix output:"
          cat $GITHUB_OUTPUT

      - name: Upload location parts
        uses: actions/upload-artifact@v4
        with:
          name: location-parts
          path: temp_locations/
          retention-days: 1

  scrape:
    needs: setup
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix: ${{fromJson(needs.setup.outputs.matrix)}}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Download location parts
        uses: actions/download-artifact@v4
        with:
          name: location-parts
          path: temp_locations

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install playwright
          python -m playwright install chromium
          python -m playwright install-deps

      - name: Verify part file
        run: |
          echo "Processing part ${{ matrix.part }}"
          echo "File exists: $(test -f temp_locations/locations_${{ matrix.part }}.csv && echo "Yes" || echo "No")"
          echo "Number of locations in this part: $(test -f temp_locations/locations_${{ matrix.part }}.csv && tail -n +2 temp_locations/locations_${{ matrix.part }}.csv | wc -l || echo "File not found")"

      - name: Run Publix scraper
        run: |
          # Copy the specific part file to use as input
          cp "temp_locations/locations_${{ matrix.part }}.csv" input_locations.csv
          
          # Run scraper with this part's locations
          python publix.py --input_file input_locations.csv --output_file "results_${{ matrix.part }}.csv" --workers 1 --headless true
          
          # Check results
          if [ -f "results_${{ matrix.part }}.csv" ]; then
            echo "Results file created with $(wc -l < results_${{ matrix.part }}.csv) lines"
            echo "Unique locations in results: $(tail -n +2 results_${{ matrix.part }}.csv | cut -d',' -f3 | sort | uniq | wc -l)"
          else
            echo "Warning: No results file was created!"
            # Create empty file to avoid errors in later steps
            echo "ID,Store ID,Location,Category,Product Name,Price,Size,Date" > "results_${{ matrix.part }}.csv"
          fi

      - name: Upload part results
        uses: actions/upload-artifact@v4
        with:
          name: publix-data-${{ matrix.part }}
          path: results_${{ matrix.part }}.csv
          retention-days: 30

  combine:
    needs: scrape
    runs-on: ubuntu-latest
    
    steps:
      - name: Download all results
        uses: actions/download-artifact@v4
        with:
          path: ./results
          pattern: publix-data-*
          merge-multiple: true
      
      - name: Combine results
        run: |
          mkdir -p combined
          DATE=$(date +%Y%m%d)
          
          # List all result files
          echo "Found result files:"
          find ./results -name "*.csv" | sort
          
          # Create header in output file
          echo "ID,Store ID,Location,Category,Product Name,Price,Size,Date" > "combined/publix_targeted_data_${DATE}_combined.csv"
          
          # Append data from all files (skipping headers)
          for file in $(find ./results -name "*.csv" | sort); do
            if [ -f "$file" ]; then
              echo "Processing $file"
              tail -n +2 "$file" >> "combined/publix_targeted_data_${DATE}_combined.csv"
            fi
          done
          
          # Count results
          TOTAL_LINES=$(wc -l < "combined/publix_targeted_data_${DATE}_combined.csv")
          DATA_LINES=$((TOTAL_LINES - 1))  # Subtract header line
          echo "Combined file created with $TOTAL_LINES lines ($DATA_LINES data rows)"
          
          # Count unique locations
          UNIQUE_LOCATIONS=$(tail -n +2 "combined/publix_targeted_data_${DATE}_combined.csv" | cut -d',' -f3 | sort | uniq | wc -l)
          echo "Unique locations in combined results: $UNIQUE_LOCATIONS"
          
          # Count unique store IDs
          UNIQUE_STORES=$(tail -n +2 "combined/publix_targeted_data_${DATE}_combined.csv" | cut -d',' -f2 | sort | uniq | wc -l)
          echo "Unique store IDs in combined results: $UNIQUE_STORES"
          
          # List the first 10 unique locations
          echo "First 10 unique locations:"
          tail -n +2 "combined/publix_targeted_data_${DATE}_combined.csv" | cut -d',' -f3 | sort | uniq | head -n 10

      - name: Upload combined results
        uses: actions/upload-artifact@v4
        with:
          name: publix-combined-data
          path: combined/publix_targeted_data_*_combined.csv
          retention-days: 90
