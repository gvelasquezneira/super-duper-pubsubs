name: Publix Data Collection

on:
  schedule:
    - cron: '1 6 * * 2'  # Runs at 6:01 AM every Tuesday
  workflow_dispatch:  # Allows manual triggering

jobs:
  setup:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.split-locations.outputs.matrix }}
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          
      - name: Split locations file into 20 parts
        id: split-locations
        run: |
          # Create a temporary directory
          mkdir -p temp_locations
          
          # Count total number of locations (excluding header)
          TOTAL=$(tail -n +2 publix_locations.csv | wc -l)
          
          # Calculate lines per file (ceiling division)
          PER_FILE=$(( (TOTAL + 19) / 20 ))
          
          # Split the file with fixed naming
          head -n 1 publix_locations.csv > header.csv
          tail -n +2 publix_locations.csv > data.csv
          
          # Split into numbered parts
          for i in {1..20}; do
            START_LINE=$(( (i-1) * PER_FILE + 1 ))
            if [ $i -lt 20 ]; then
              END_LINE=$(( i * PER_FILE ))
              sed -n "${START_LINE},${END_LINE}p" data.csv > "temp_locations/part_${i}.csv"
            else
              # Last part gets all remaining lines
              sed -n "${START_LINE},\$p" data.csv > "temp_locations/part_${i}.csv"
            fi
            # Add header
            cat header.csv "temp_locations/part_${i}.csv" > "temp_locations/locations_part_${i}.csv"
          done
          
          # Create matrix output for 20 parts
          echo "matrix={\"part\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]}" >> $GITHUB_OUTPUT
          
      - name: Upload split files
        uses: actions/upload-artifact@v4
        with:
          name: split-location-files
          path: temp_locations
          retention-days: 1

  scrape:
    needs: setup
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix: ${{fromJson(needs.setup.outputs.matrix)}}
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          
      - name: Download split location files
        uses: actions/download-artifact@v4
        with:
          name: split-location-files
          path: temp_locations
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install playwright
          python -m playwright install chromium
          python -m playwright install-deps
          
      - name: Create part-specific locations file
        run: |
          # Use the specific part file with consistent naming
          cp "temp_locations/locations_part_${{ matrix.part }}.csv" publix_locations.csv
          
          # Check how many locations we're processing
          echo "Processing these locations for part ${{ matrix.part }}:"
          cat publix_locations.csv
          
      - name: Run Publix scraper
        run: |
          python publix.py --workers 1
          
      - name: Upload results
        uses: actions/upload-artifact@v4
        with:
          name: publix-data-part-${{ matrix.part }}
          path: publix_targeted_data_*.csv
          retention-days: 30

  combine:
    needs: scrape
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          
      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts
          
      - name: Combine results
        run: |
          # Create output directory
          mkdir -p combined
          
          # Get today's date in YYYYMMDD format
          DATE=$(date +%Y%m%d)
          
          # Create a combined file with headers
          echo 'Location,Category,Product Name,Price,Ounces,Date' > combined/publix_targeted_data_${DATE}_combined.csv
          
          # Append all data files (skipping headers)
          for part in {1..20}; do
            # Look for the files in the artifact directories
            FILE=$(find artifacts/publix-data-part-$part -name "publix_targeted_data_*.csv" -type f | head -1)
            if [ -f "$FILE" ]; then
              echo "Adding data from $FILE"
              tail -n +2 "$FILE" >> combined/publix_targeted_data_${DATE}_combined.csv
            else
              echo "Warning: No data file found for part $part"
            fi
          done
          
          # Check the combined file
          echo "Combined file created with $(wc -l < combined/publix_targeted_data_${DATE}_combined.csv) lines"
          
      - name: Upload combined results
        uses: actions/upload-artifact@v4
        with:
          name: publix-combined-data
          path: combined/publix_targeted_data_*_combined.csv
          retention-days: 90
